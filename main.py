import json
import os
import time
from typing import TypedDict

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from langgraph.graph import StateGraph
from google.generativeai import configure, GenerativeModel

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---

configure(api_key=GEMINI_API_KEY)
model = GenerativeModel("gemini-2.5-flash")

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# --- State ---
class AgentState(TypedDict):
    inn: str
    egrul_data: dict
    raw_financial_data: dict
    downloaded_xml_paths: list
    plaintiff_cases: list
    respondent_cases: list
    markdown_report: str

# --- –§—É–Ω–∫—Ü–∏—è 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ï–ì–†–Æ–õ ---
def fetch_egrul_data(state: AgentState) -> AgentState:
    inn = state["inn"]
    url = EGRUL_API_URL_TEMPLATE.format(inn=inn)
    
    print(f"üìã –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –ï–ì–†–Æ–õ –¥–ª—è –ò–ù–ù {inn}...")
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        data = response.json()
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –ï–ì–†–Æ–õ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        data = {"error": f"Failed to fetch EGRUL: {str(e)}"}
        print(f"‚ùå –û—à–∏–±–∫–∞ –ï–ì–†–Æ–õ: {str(e)}")

    return {"egrul_data": data}

# --- –§—É–Ω–∫—Ü–∏—è 2: –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---
def fetch_financial_data(state: AgentState) -> AgentState:
    inn = state["inn"]
    url = FIN_API_URL_TEMPLATE.format(inn=inn)
    
    print(f"üí∞ –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ò–ù–ù {inn}...")

    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        print("‚úÖ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        data = {"error": f"Failed to fetch financial: {str(e)}"}
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤: {str(e)}")

    return {"raw_financial_data": data}

# --- –§—É–Ω–∫—Ü–∏—è 3: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ ---
def download_xml_reports(state: AgentState) -> AgentState:
    inn = state["inn"]
    out_dir = f"reports_{inn}"
    os.makedirs(out_dir, exist_ok=True)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–∞—é –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫—É—é –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ò–ù–ù {inn}...")
    print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ 5 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º...")
    time.sleep(5)
    
    for attempt in range(3):
        try:
            resp = requests.get(
                f"{BASE_BOH_URL}ls.php",
                params={"inn": inn},
                headers=HEADERS,
                timeout=15
            )
            resp.raise_for_status()
            break
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                wait = 20 * (attempt + 1)
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ 429. –ñ–¥–µ–º {wait} —Å–µ–∫—É–Ω–¥...")
                time.sleep(wait)
                if attempt == 2:
                    return {"downloaded_xml_paths": [{"error": "429 –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫"}]}
            else:
                return {"downloaded_xml_paths": [{"error": f"HTTP Error: {str(e)}"}]}
        except Exception as e:
            return {"downloaded_xml_paths": [{"error": f"Request failed: {str(e)}"}]}

    soup = BeautifulSoup(resp.text, "html.parser")
    links = soup.find_all('a', string='–°–∫–∞—á–∞—Ç—å')
    
    if not links:
        print("‚ö†Ô∏è –ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return {"downloaded_xml_paths": []}
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(links)}")
    downloaded = []

    for idx, link in enumerate(links, 1):
        href = link.get("href")
        if not href:
            continue

        if idx > 1:
            print(f"‚è≥ –ñ–¥–µ–º 10 —Å–µ–∫—É–Ω–¥...")
            time.sleep(10)
        
        url = urljoin(BASE_BOH_URL, href)
        filename = f"report_{inn}_{idx}.pdf"
        path = os.path.join(out_dir, filename)
        
        print(f"üì• [{idx}/{len(links)}]: {filename}")
        
        for file_attempt in range(3):
            try:
                r = requests.get(url, headers=HEADERS, timeout=15)
                r.raise_for_status()
                with open(path, "wb") as f:
                    f.write(r.content)
                downloaded.append(path)
                print(f"‚úÖ OK")
                break
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    wait = 20 * (file_attempt + 1)
                    print(f"‚ö†Ô∏è 429. –ñ–¥–µ–º {wait} —Å–µ–∫...")
                    time.sleep(wait)
                else:
                    break

    print(f"‚úÖ –°–∫–∞—á–∞–Ω–æ {len(downloaded)} —Ñ–∞–π–ª–æ–≤")
    return {"downloaded_xml_paths": downloaded}

# --- –§—É–Ω–∫—Ü–∏—è 4: –ü–æ–ª—É—á–µ–Ω–∏–µ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö –¥–µ–ª ---
def get_arbitration_cases_by_inn(inn: str, api_key: str) -> dict:
    url = ARBITRATION_API_URL
    params = {
        'key': api_key,
        'Inn': inn,
        'InnType': 'Any'
    }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        if data.get('Success') != 1:
            error_msg = data.get('error', 'API returned Success != 1')
            print(f"‚ö†Ô∏è {error_msg}")
        return data

    except requests.exceptions.HTTPError as e:
        return {"error": f"HTTP Error: {e}", "details": response.text if response else None}
    except requests.exceptions.RequestException as e:
        return {"error": f"Request Error: {str(e)}"}
    except ValueError:
        return {"error": "Response is not valid JSON", "content": response.text}

def parse_arbitration_cases(arb_data: dict, company_inn: str):
    cases = arb_data.get("Cases", [])
    plaintiff_cases = []
    respondent_cases = []

    for case in cases:
        is_plaintiff = False
        is_respondent = False

        for plaintiff in case.get("Plaintiffs") or []:
            if plaintiff and plaintiff.get("Inn") == company_inn:
                is_plaintiff = True
                break

        for respondent in case.get("Respondents") or []:
            if respondent and respondent.get("Inn") == company_inn:
                is_respondent = True
                break

        if is_plaintiff:
            plaintiff_cases.append(case)
        if is_respondent:
            respondent_cases.append(case)

    return plaintiff_cases, respondent_cases

def fetch_arbitration_data(state: AgentState) -> AgentState:
    inn = state["inn"]
    
    print(f"‚öñÔ∏è –ó–∞–≥—Ä—É–∂–∞—é —Å—É–¥–µ–±–Ω—ã–µ –¥–µ–ª–∞ –¥–ª—è –ò–ù–ù {inn}...")
    
    raw_arb_data = get_arbitration_cases_by_inn(inn, ARBITRATION_API_KEY)
    plt_cases, resp_cases = parse_arbitration_cases(raw_arb_data, inn)
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–µ–ª: –∏—Å—Ç–µ—Ü={len(plt_cases)}, –æ—Ç–≤–µ—Ç—á–∏–∫={len(resp_cases)}")

    return {
        "plaintiff_cases": plt_cases,
        "respondent_cases": resp_cases
    }

# --- –§—É–Ω–∫—Ü–∏—è 5: –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ ---
def read_xml_files_as_text(paths):
    texts = []
    for path in paths:
        if isinstance(path, dict) and "error" in path:
            texts.append(path["error"])
            continue
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            texts.append(content)
        except:
            texts.append(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {os.path.basename(path)}")
    return "\n\n".join(texts)

# --- –§—É–Ω–∫—Ü–∏—è 6: –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ ---
def merge_and_analyze(state: AgentState) -> AgentState:
    egrul_data = state["egrul_data"]
    fin_data = state["raw_financial_data"]
    xml_texts = read_xml_files_as_text(state["downloaded_xml_paths"])
    plt_cases = state["plaintiff_cases"]
    resp_cases = state["respondent_cases"]

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
    if len(xml_texts) > 50000:
        xml_texts = xml_texts[:50000] + "\n\n[–û–ë–†–ï–ó–ê–ù–û]"

    prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º—É –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∫–æ–º–ø–∞–Ω–∏–π.

–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π markdown-–æ—Ç—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

# –°–¢–†–£–ö–¢–£–†–ê –û–¢–ß–ï–¢–ê:

## 1. üè¢ –ö–û–†–ü–û–†–ê–¢–ò–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø (–ï–ì–†–Æ–õ)

### –û—Å–Ω–æ–≤–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è:
- –ò–ù–ù, –û–ì–†–ù
- –ü–æ–ª–Ω–æ–µ –∏ –∫—Ä–∞—Ç–∫–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
- –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤–∞—è —Ñ–æ—Ä–º–∞
- –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
- –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–∞–Ω–∏–∏
- –£—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª

### –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å:
–ü–æ–ª–Ω—ã–π –∞–¥—Ä–µ—Å

### üë• –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ:
–¢–∞–±–ª–∏—Ü–∞ —Å –§–ò–û, –¥–æ–ª–∂–Ω–æ—Å—Ç—è–º–∏, –¥–∞—Ç–∞–º–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è

### üíº –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–ª–∞–¥–µ–Ω–∏—è:

**–£—á—Ä–µ–¥–∏—Ç–µ–ª–∏:**
–¢–∞–±–ª–∏—Ü–∞ —Å –∏–º–µ–Ω–∞–º–∏/–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏, –¥–æ–ª—è–º–∏, –ò–ù–ù (–¥–ª—è —é—Ä–ª–∏—Ü)

**Mermaid –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**
\`\`\`mermaid
graph TD
    A[–ö–æ–º–ø–∞–Ω–∏—è<br/>–ò–ù–ù: XXX]
    B[–£—á—Ä–µ–¥–∏—Ç–µ–ª—å 1<br/>–î–æ–ª—è: XX%]
    C[–£—á—Ä–µ–¥–∏—Ç–µ–ª—å 2<br/>–î–æ–ª—è: XX%]
    
    B --> A
    C --> A
\`\`\`

**–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**
- –ö—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–Ω–∏—é (–∫–æ–Ω–µ—á–Ω—ã–µ –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä—ã)
- –ï—Å—Ç—å –ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ª–∏—Ü–∞
- –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–ª–∞–¥–µ–Ω–∏—è

### üè≠ –í–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–û–ö–í–≠–î):
–û—Å–Ω–æ–≤–Ω–æ–π –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ

### ‚ö†Ô∏è –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ä–∏—Å–∫–∏:
- –ú–∞—Å—Å–æ–≤—ã–π –∞–¥—Ä–µ—Å
- –ù–æ–º–∏–Ω–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä
- –ù–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
- –î—Ä—É–≥–∏–µ –∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏

---

## 2. üí∞ –§–ò–ù–ê–ù–°–û–í–´–ô –ê–ù–ê–õ–ò–ó

### –ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:
–ò–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –≤—ã—Ä—É—á–∫–∞, –ø—Ä–∏–±—ã–ª—å, –∞–∫—Ç–∏–≤—ã, –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞

### –î–∏–Ω–∞–º–∏–∫–∞:
–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ –≥–æ–¥–∞–º/–ø–µ—Ä–∏–æ–¥–∞–º

### –§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
–ü–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å, —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å

---

## 3. üìä –ë–£–•–ì–ê–õ–¢–ï–†–°–ö–ê–Ø –û–¢–ß–ï–¢–ù–û–°–¢–¨

### –ê–Ω–∞–ª–∏–∑ –æ—Ç—á–µ—Ç–æ–≤:
- –ö–∞–∫–∏–µ —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã
- –ü–µ—Ä–∏–æ–¥—ã –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
- –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –±–∞–ª–∞–Ω—Å–∞
- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

### –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
–ê–Ω–æ–º–∞–ª–∏–∏, –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∏–∑–º–µ–Ω–µ–Ω–∏—è

---

## 4. ‚öñÔ∏è –°–£–î–ï–ë–ù–ê–Ø –ê–ö–¢–ò–í–ù–û–°–¢–¨

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
- –í—Å–µ–≥–æ –¥–µ–ª: XX
- –ö–æ–º–ø–∞–Ω–∏—è –∫–∞–∫ –∏—Å—Ç–µ—Ü: XX –¥–µ–ª
- –ö–æ–º–ø–∞–Ω–∏—è –∫–∞–∫ –æ—Ç–≤–µ—Ç—á–∏–∫: XX –¥–µ–ª

### –î–µ–ª–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å—Ç—Ü–∞:
–¢–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö –¥–µ–ª (–Ω–æ–º–µ—Ä, —Å—É—Ç—å, —Å—É–º–º–∞, —Å—Ç–∞—Ç—É—Å)

### –î–µ–ª–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç—á–∏–∫–∞:
–¢–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö –¥–µ–ª

### –°—É–¥–µ–±–Ω—ã–µ —Ä–∏—Å–∫–∏:
- –¢–∏–ø–∏—á–Ω—ã–µ —Å–ø–æ—Ä—ã
- –ö—Ä—É–ø–Ω—ã–µ –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏
- –ü—Ä–æ–∏–≥—Ä–∞–Ω–Ω—ã–µ –¥–µ–ª–∞
- –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—É–¥–µ–±–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏

---

## 5. üéØ –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –û–¶–ï–ù–ö–ê –ò –í–´–í–û–î–´

### –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
### –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
### –ö–ª—é—á–µ–≤—ã–µ —Ä–∏—Å–∫–∏:
### –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

**–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:** (–ø–æ —à–∫–∞–ª–µ 1-10)

---

# –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:

## –î–∞–Ω–Ω—ã–µ –ï–ì–†–Æ–õ:
{json.dumps(egrul_data, ensure_ascii=False, indent=2)}

## –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:
{json.dumps(fin_data, ensure_ascii=False, indent=2)}

## –ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å:
{xml_texts}

## –°—É–¥–µ–±–Ω—ã–µ –¥–µ–ª–∞ (–∏—Å—Ç–µ—Ü):
{json.dumps(plt_cases, ensure_ascii=False, indent=2)}

## –°—É–¥–µ–±–Ω—ã–µ –¥–µ–ª–∞ (–æ—Ç–≤–µ—Ç—á–∏–∫):
{json.dumps(resp_cases, ensure_ascii=False, indent=2)}

---

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã –≤ markdown
2. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–π Mermaid –¥–∏–∞–≥—Ä–∞–º–º—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–ª–∞–¥–µ–Ω–∏—è
3. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–π —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã
4. –í—ã–¥–µ–ª–∏ —Ä–∏—Å–∫–∏ –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º
5. –î–∞–π —á–µ—Ç–∫–∏–µ –≤—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
"""

    print("\nü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —á–µ—Ä–µ–∑ Gemini...")
    
    try:
        time.sleep(3)
        response = model.generate_content(prompt)
        report = response.text or "–û—Ç—á–µ—Ç –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω."
        print("‚úÖ –û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤")
    except Exception as e:
        report = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}"
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

    return {"markdown_report": report}

# --- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ ---
workflow = StateGraph(AgentState)

workflow.add_node("fetch_egrul", fetch_egrul_data)
workflow.add_node("fetch_financial", fetch_financial_data)
workflow.add_node("download_xml", download_xml_reports)
workflow.add_node("fetch_arbitration", fetch_arbitration_data)
workflow.add_node("merge_and_analyze", merge_and_analyze)

workflow.set_entry_point("fetch_egrul")
workflow.add_edge("fetch_egrul", "fetch_financial")
workflow.add_edge("fetch_financial", "download_xml")
workflow.add_edge("download_xml", "fetch_arbitration")
workflow.add_edge("fetch_arbitration", "merge_and_analyze")
workflow.add_edge("merge_and_analyze", "__end__")

app = workflow.compile()

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    print("="*80)
    print("üìä –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–ê–ù–ò–ò")
    print("="*80)
    
    user_input_inn = input("\nüî¢ –í–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∫–æ–º–ø–∞–Ω–∏–∏: ").strip()
    
    initial_state = {
        "inn": user_input_inn,
        "egrul_data": {},
        "raw_financial_data": {},
        "downloaded_xml_paths": [],
        "plaintiff_cases": [],
        "respondent_cases": [],
        "markdown_report": ""
    }

    print("\n" + "="*80)
    print("üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê")
    print("="*80 + "\n")

    result = app.invoke(initial_state)

    print("\n" + "="*80)
    print("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("="*80 + "\n")
    print(result["markdown_report"])

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    out_dir = f"reports_{user_input_inn}"
    os.makedirs(out_dir, exist_ok=True)
    report_file = os.path.join(out_dir, "final_corporate_report.md")
    
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(result["markdown_report"])
    
    print("\n" + "="*80)
    print(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    print("="*80)
